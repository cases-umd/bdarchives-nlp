{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define directory path and entity type\n",
    "\n",
    "my_directory = \"/directory/of/file/collection/\"\n",
    "output_loc = \"/directory/for/output/csvs/\"\n",
    "ent_type = \"PERSON\"\n",
    "\n",
    "### entity type can be \"PERSON\", \"NORP\", \"ORG\", \"GPE\", etc.\n",
    "### https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import PyPDF2\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import docx2txt\n",
    "import docx\n",
    "import codecs\n",
    "import subprocess\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "os.chdir(my_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/erhiggs/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##run if needed\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walking directory and compiling text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "allfiles = []\n",
    "onlypdf = []\n",
    "onlydoc = []\n",
    "onlydocx = []\n",
    "onlytxt = []\n",
    "\n",
    "for root, dirs, files in os.walk(my_directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".doc\"):\n",
    "            allfiles.append(os.path.join(root, file))\n",
    "            onlydoc.append(os.path.join(root, file))\n",
    "            \n",
    "for root, dirs, files in os.walk(my_directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".docx\"):\n",
    "            allfiles.append(os.path.join(root, file))\n",
    "            onlydocx.append(os.path.join(root, file)) \n",
    "            \n",
    "for root, dirs, files in os.walk(my_directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            allfiles.append(os.path.join(root, file))\n",
    "            onlytxt.append(os.path.join(root, file))\n",
    "            \n",
    "for root, dirs, files in os.walk(my_directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pdf\"):\n",
    "            allfiles.append(os.path.join(root, file))\n",
    "            onlypdf.append(os.path.join(root, file))\n",
    "            \n",
    "print 'files total:', len(allfiles)\n",
    "print 'doc:', len(onlydoc)\n",
    "print 'docx:', len(onlydocx)\n",
    "print 'pdf:', len(onlypdf)\n",
    "print 'txt:', len(onlytxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract text and compile entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ent = []\n",
    "pdf_ent = []\n",
    "doc_ent = []\n",
    "docx_ent = []\n",
    "txt_ent = []\n",
    "\n",
    "## the separate lists aren't necessary for this but keeping them anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ents from pdf\n",
    "\n",
    "for filename in onlypdf: \n",
    "    try:\n",
    "        pdfFileObj = open(filename, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        for i in range(pdfReader.numPages):\n",
    "            pageObj = pdfReader.getPage(i)\n",
    "            pagetext = ((pageObj.extractText()).replace('\\n', ' '))\n",
    "            pdfdoc = nlp(pagetext)\n",
    "            for ent in pdfdoc.ents:\n",
    "                entpair = (ent.text, ent.label_)\n",
    "                pdf_ent.append(entpair)\n",
    "                all_ent.append(entpair)\n",
    "    except: \n",
    "        pass\n",
    "    \n",
    "print 'entities from .pdf:',(len(doc_ent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7575666\n"
     ]
    }
   ],
   "source": [
    "##ents from doc\n",
    "\n",
    "for filename in onlydoc: \n",
    "    try:\n",
    "        cmd = ['antiword', filename]\n",
    "        p = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n",
    "        (output, err) = p.communicate()\n",
    "        uniout = unicode(output, \"utf-8\")\n",
    "        doc = nlp(uniout)\n",
    "        for ent in doc.ents:\n",
    "            entpair = (ent.text, ent.label_)\n",
    "            doc_ent.append(entpair)\n",
    "            all_ent.append(entpair)\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "print 'entities from .doc:',(len(doc_ent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18718\n"
     ]
    }
   ],
   "source": [
    "##ents from docx\n",
    "\n",
    "for filename in onlydocx: \n",
    "    try:\n",
    "        pagetext = docx2txt.process(filename)\n",
    "        docxdoc = nlp(pagetext)\n",
    "        for ent in docxdoc.ents:\n",
    "            entpair = (ent.text, ent.label_)\n",
    "            docx_ent.append(entpair)\n",
    "            all_ent.append(entpair)\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "print 'entities from .docx:', (len(docx_ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22473674\n"
     ]
    }
   ],
   "source": [
    "##ents from txt\n",
    "\n",
    "for filename in onlytxt: \n",
    "    try:\n",
    "        with codecs.open(filename, 'r', encoding='utf-8') as myfile:\n",
    "            pagetext=myfile.read().replace('\\n', ' ')\n",
    "            txtdoc = nlp(pagetext)\n",
    "            for ent in txtdoc.ents:\n",
    "                entpair = (ent.text, ent.label_)\n",
    "                txt_ent.append(entpair)\n",
    "                all_ent.append(entpair)\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "print 'entities from .txt:', (len(txt_ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all entities: 22473674\n"
     ]
    }
   ],
   "source": [
    "print 'all entities:', len(all_ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify entity type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4833073\n"
     ]
    }
   ],
   "source": [
    "entlist = [x for x in all_ent if ent_type in x]\n",
    "print(len(entlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4060347\n"
     ]
    }
   ],
   "source": [
    "filter_entlist = [x for x in entlist if (x[0])[0].isalpha() and (x[0])[-1].isalpha()] \n",
    "\n",
    "## this will limit list to entities that start and end with an alphanumerical character\n",
    "\n",
    "print(len(filter_entlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'II l\\\\_\\\\  ', u'PERSON'), (u't]\\\\', u'PERSON'), (u\"l'lll\\u2018ll\\\\lllll\\xa3\", u'PERSON'), (u'                   McGrane', u'PERSON'), (u'the Graham Pa-', u'PERSON'), (u'RETURN  ', u'PERSON'), (u\"Ourel netgl'JJUf'\", u'PERSON'), (u'Wlll d0', u'PERSON'), (u' Knowland', u'PERSON'), (u'\\xa2\\u20ac\\u20ac4\\u20ac\\u20ac\\u20ac\\u20ac\\u20ac\\u20ac\\u20ac\\u20ac\\u20ac\\u20ac\\u20ac\\u20ac\\u20ac\\u20ac\\u20ac\\u20ac\\u20ac\\u20ac\\u20ac\\u20ac\\u20ac        VVVVVVVVVVVVVVVVVVVVVVV     ', u'PERSON'), (u'Hal C. Hart-', u'PERSON'), (u'\\\\\\\\L\\u2018L\\u2018lsetltlst', u'PERSON'), (u'                                                                                                                                                                                                                                                                                                                                                                                                                       ', u'PERSON'), (u'Izar L. H.  ', u'PERSON'), (u'EIfltI  ', u'PERSON'), (u'\\\\lbert R. Ilutler  ', u'PERSON'), (u'               W s 1 \\u2018', u'PERSON'), (u'                                                    Terps', u'PERSON'), (u'Josh 1.111', u'PERSON'), (u'House        ', u'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "print((list(set(entlist) - set(filter_entlist)))[0:20])\n",
    "\n",
    "## check this output for data loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1953791\n"
     ]
    }
   ],
   "source": [
    "## filter again to only include names w/ more than 1 word\n",
    "\n",
    "filter_entlist2 = [x for x in entlist if (x[0])[0].isalpha() and (x[0])[-1].isalpha() and ' ' in x[0]]\n",
    "print(len(filter_entlist2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return top entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(output_loc)\n",
    "\n",
    "namecount = Counter(filter_entlist)\n",
    "fullnamecount = Counter(filter_entlist2)\n",
    "commonnames = [x for x in fullnamecount.most_common() if x[1] > 5]\n",
    "commonall = [x for x in namecount.most_common() if x[1] > 5]\n",
    "\n",
    "entities_table = []\n",
    "\n",
    "for name in commonnames:\n",
    "    row = [(name[0])[0].encode('utf-8'), name[1]]\n",
    "    entities_table.append(row)\n",
    "\n",
    "out_path = \"entities_fullnames.csv\"\n",
    "\n",
    "header = ['Name', 'Frequency']\n",
    "\n",
    "with open(out_path, 'w') as fo:\n",
    "    csv_writer = csv.writer(fo)\n",
    "    csv_writer.writerow(header)\n",
    "    csv_writer.writerows(entities_table)\n",
    "    \n",
    "entities_table2 = []\n",
    "\n",
    "for name in commonall:\n",
    "    row = [(name[0])[0].encode('utf-8'), name[1]]\n",
    "    entities_table2.append(row)\n",
    "\n",
    "out_path = \"names_all.csv\"\n",
    "\n",
    "header = ['Name', 'Frequency']\n",
    "\n",
    "with open(out_path, 'w') as fo:\n",
    "    csv_writer = csv.writer(fo)\n",
    "    csv_writer.writerow(header)\n",
    "    csv_writer.writerows(entities_table2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
