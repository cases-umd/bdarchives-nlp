{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the English Language Model\n",
    "\n",
    "If you have not already done so, you will need to run this code to download the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /home/jansen/.local/share/virtualenvs/bdarchives-nlp-worlW0cl/lib/python3.6/site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /home/jansen/.local/share/virtualenvs/bdarchives-nlp-worlW0cl/lib/python3.6/site-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/jansen/.local/share/virtualenvs/bdarchives-nlp-worlW0cl/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.44.1)\n",
      "Requirement already satisfied: setuptools in /home/jansen/.local/share/virtualenvs/bdarchives-nlp-worlW0cl/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/jansen/.local/share/virtualenvs/bdarchives-nlp-worlW0cl/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in /home/jansen/.local/share/virtualenvs/bdarchives-nlp-worlW0cl/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/jansen/.local/share/virtualenvs/bdarchives-nlp-worlW0cl/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/jansen/.local/share/virtualenvs/bdarchives-nlp-worlW0cl/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/jansen/.local/share/virtualenvs/bdarchives-nlp-worlW0cl/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/jansen/.local/share/virtualenvs/bdarchives-nlp-worlW0cl/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/jansen/.local/share/virtualenvs/bdarchives-nlp-worlW0cl/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/jansen/.local/share/virtualenvs/bdarchives-nlp-worlW0cl/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/jansen/.local/share/virtualenvs/bdarchives-nlp-worlW0cl/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/jansen/.local/share/virtualenvs/bdarchives-nlp-worlW0cl/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/jansen/.local/share/virtualenvs/bdarchives-nlp-worlW0cl/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/jansen/.local/share/virtualenvs/bdarchives-nlp-worlW0cl/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/jansen/.local/share/virtualenvs/bdarchives-nlp-worlW0cl/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jansen/.local/share/virtualenvs/bdarchives-nlp-worlW0cl/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/jansen/.local/share/virtualenvs/bdarchives-nlp-worlW0cl/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/jansen/.local/share/virtualenvs/bdarchives-nlp-worlW0cl/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jansen/.local/share/virtualenvs/bdarchives-nlp-worlW0cl/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define directory path and entity type\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "data_loc = cwd + \"/data\"\n",
    "output_loc = cwd + \"/output/\"\n",
    "ent_type = \"PERSON\"\n",
    "\n",
    "### entity type can be \"PERSON\", \"NORP\", \"ORG\", \"GPE\", etc.\n",
    "### https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#import PyPDF2\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "#import docx2txt\n",
    "#import docx\n",
    "import codecs\n",
    "import subprocess\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walking directory and compiling text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files total: 4 \n",
      "doc: 0\n",
      "docx: 0\n",
      "pdf: 0\n",
      "txt: 4\n"
     ]
    }
   ],
   "source": [
    "allfiles = []\n",
    "onlypdf = []\n",
    "onlydoc = []\n",
    "onlydocx = []\n",
    "onlytxt = []\n",
    "\n",
    "for root, dirs, files in os.walk(data_loc):\n",
    "    for file in files:\n",
    "        allfiles.append(os.path.join(root, file))\n",
    "        if file.endswith(\".doc\"):\n",
    "           onlydoc.append(os.path.join(root, file))\n",
    "        elif file.endswith(\".docx\"):\n",
    "            onlydocx.append(os.path.join(root, file))     \n",
    "        elif file.endswith(\".txt\"):\n",
    "            onlytxt.append(os.path.join(root, file))\n",
    "        elif file.endswith(\".pdf\"):\n",
    "            onlypdf.append(os.path.join(root, file))\n",
    "            \n",
    "print('files total: %d ' % len(allfiles))\n",
    "print('doc: %d' % len(onlydoc))\n",
    "print('docx: %d' % len(onlydocx))\n",
    "print('pdf: %d' % len(onlypdf))\n",
    "print('txt: %d' % len(onlytxt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract text and compile entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ent = []\n",
    "pdf_ent = []\n",
    "doc_ent = []\n",
    "docx_ent = []\n",
    "txt_ent = []\n",
    "\n",
    "## the separate lists aren't necessary for this but keeping them anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities from .pdf: 0\n"
     ]
    }
   ],
   "source": [
    "##ents from pdf\n",
    "\n",
    "for filename in onlypdf: \n",
    "    try:\n",
    "        pdfFileObj = open(filename, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        for i in range(pdfReader.numPages):\n",
    "            pageObj = pdfReader.getPage(i)\n",
    "            pagetext = ((pageObj.extractText()).replace('\\n', ' '))\n",
    "            pdfdoc = nlp(pagetext)\n",
    "            for ent in pdfdoc.ents:\n",
    "                entpair = (ent.text, ent.label_)\n",
    "                pdf_ent.append(entpair)\n",
    "                all_ent.append(entpair)\n",
    "    except: \n",
    "        pass\n",
    "    \n",
    "print('entities from .pdf: %d' % len(doc_ent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities from .doc: 0\n"
     ]
    }
   ],
   "source": [
    "##ents from doc\n",
    "\n",
    "for filename in onlydoc: \n",
    "    try:\n",
    "        cmd = ['antiword', filename]\n",
    "        p = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n",
    "        (output, err) = p.communicate()\n",
    "        uniout = unicode(output, \"utf-8\")\n",
    "        doc = nlp(uniout)\n",
    "        for ent in doc.ents:\n",
    "            entpair = (ent.text, ent.label_)\n",
    "            doc_ent.append(entpair)\n",
    "            all_ent.append(entpair)\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "print('entities from .doc: %d' % len(doc_ent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities from .docx: 0\n"
     ]
    }
   ],
   "source": [
    "##ents from docx\n",
    "\n",
    "for filename in onlydocx: \n",
    "    try:\n",
    "        pagetext = docx2txt.process(filename)\n",
    "        docxdoc = nlp(pagetext)\n",
    "        for ent in docxdoc.ents:\n",
    "            entpair = (ent.text, ent.label_)\n",
    "            docx_ent.append(entpair)\n",
    "            all_ent.append(entpair)\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "print('entities from .docx: %d' % len(docx_ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities from .txt: 230\n"
     ]
    }
   ],
   "source": [
    "##ents from txt\n",
    "\n",
    "for filename in onlytxt: \n",
    "    try:\n",
    "        with codecs.open(filename, 'r', encoding='utf-8') as myfile:\n",
    "            pagetext=myfile.read().replace('\\n', ' ')\n",
    "            txtdoc = nlp(pagetext)\n",
    "            for ent in txtdoc.ents:\n",
    "                entpair = (ent.text, ent.label_)\n",
    "                txt_ent.append(entpair)\n",
    "                all_ent.append(entpair)\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "print('entities from .txt: %d' % len(txt_ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total entities: 230\n"
     ]
    }
   ],
   "source": [
    "print('total entities: %d' % len(all_ent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify entity type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "entlist = [x for x in all_ent if ent_type in x]\n",
    "print(len(entlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "filter_entlist = [x for x in entlist if (x[0])[0].isalpha() and (x[0])[-1].isalpha()] \n",
    "\n",
    "## this will limit list to entities that start and end with an alphanumerical character\n",
    "\n",
    "print(len(filter_entlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Willie\\t\\t', 'PERSON'), ('James\\t', 'PERSON'), ('Mary\\t', 'PERSON'), ('Alice\\t\\t', 'PERSON'), ('Carl\\t', 'PERSON'), ('Abernethy\\t', 'PERSON'), ('Joseph\\t', 'PERSON'), ('Jennie\\t\\t', 'PERSON'), ('Abernethy\\t\\t\\t', 'PERSON'), ('William\\t', 'PERSON'), ('Frances\\t', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "print((list(set(entlist) - set(filter_entlist)))[0:20])\n",
    "\n",
    "## check this output for data loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jas M', 'PERSON'), ('s McDowell', 'PERSON'), ('s McDowell', 'PERSON'), ('Fire Ext', 'PERSON'), ('w Palmer', 'PERSON'), ('Henry Miller', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "## filter again to only include names w/ more than 1 word\n",
    "\n",
    "filter_entlist2 = [x for x in entlist if (x[0])[0].isalpha() and (x[0])[-1].isalpha() and ' ' in x[0]]\n",
    "print(filter_entlist2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return top entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_loc)\n",
    "os.chdir(output_loc)\n",
    "\n",
    "\n",
    "namecount = Counter(filter_entlist)\n",
    "fullnamecount = Counter(filter_entlist2)\n",
    "commonnames = [x for x in fullnamecount.most_common() if x[1] > 5]\n",
    "commonall = [x for x in namecount.most_common() if x[1] > 5]\n",
    "\n",
    "entities_table = []\n",
    "\n",
    "for name in commonnames:\n",
    "    row = [(name[0])[0].encode('utf-8'), name[1]]\n",
    "    entities_table.append(row)\n",
    "\n",
    "out_path = \"entities_fullnames.csv\"\n",
    "\n",
    "header = ['Name', 'Frequency']\n",
    "\n",
    "with open(out_path, 'w') as fo:\n",
    "    csv_writer = csv.writer(fo)\n",
    "    csv_writer.writerow(header)\n",
    "    csv_writer.writerows(entities_table)\n",
    "    \n",
    "entities_table2 = []\n",
    "\n",
    "for name in commonall:\n",
    "    row = [(name[0])[0].encode('utf-8'), name[1]]\n",
    "    entities_table2.append(row)\n",
    "\n",
    "out_path = \"names_all.csv\"\n",
    "\n",
    "header = ['Name', 'Frequency']\n",
    "\n",
    "with open(out_path, 'w') as fo:\n",
    "    csv_writer = csv.writer(fo)\n",
    "    csv_writer.writerow(header)\n",
    "    csv_writer.writerows(entities_table2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
